2025-02-18 11:16:04,196 - INFO - Loading and preprocessing data...
2025-02-18 11:16:04,197 - INFO - Loading ratings data...
2025-02-18 11:16:29,933 - INFO - 
Validating ratings DataFrame:
2025-02-18 11:16:33,466 - INFO - Shape: 20000263 rows, 4 columns
2025-02-18 11:18:44,036 - INFO - Loading movies data...
2025-02-18 11:18:44,694 - INFO - 
Validating movies DataFrame:
2025-02-18 11:18:44,860 - INFO - Shape: 27278 rows, 3 columns
2025-02-18 11:18:48,122 - INFO - Loading tags data...
2025-02-18 11:18:49,034 - INFO - 
Validating tags DataFrame:
2025-02-18 11:18:49,325 - INFO - Shape: 465564 rows, 4 columns
2025-02-18 11:18:51,231 - WARNING - Column timestamp has 3 null values
2025-02-18 11:18:54,979 - INFO - Loading genome scores data...
2025-02-18 11:19:03,901 - INFO - 
Validating genome scores DataFrame:
2025-02-18 11:19:05,755 - INFO - Shape: 11709768 rows, 3 columns
2025-02-18 11:20:00,748 - INFO - Loading genome tags data...
2025-02-18 11:20:01,065 - INFO - 
Validating genome tags DataFrame:
2025-02-18 11:20:01,195 - INFO - Shape: 1128 rows, 2 columns
2025-02-18 11:20:02,712 - INFO - 
Dataset sizes:
2025-02-18 11:20:06,925 - INFO - Ratings: 20,000,263 rows
2025-02-18 11:20:07,121 - INFO - Movies: 27,278 rows
2025-02-18 11:20:07,437 - INFO - Tags: 465,564 rows
2025-02-18 11:20:09,240 - INFO - Genome Scores: 11,709,768 rows
2025-02-18 11:20:09,346 - INFO - Genome Tags: 1,128 rows
2025-02-18 11:20:09,348 - INFO - Analyzing data...
2025-02-18 11:20:09,488 - INFO - 
Movie rating statistics:
2025-02-18 11:20:22,124 - INFO - 
Top rated movies:
2025-02-18 11:20:22,125 - ERROR - An error occurred: 'DataLoader' object has no attribute 'get_top_rated_movies'
Traceback (most recent call last):
  File "C:\Users\efeba\BIL401Proje\src\main.py", line 126, in main
    analyze_data(data_loader, ratings_df, movies_df)
  File "C:\Users\efeba\BIL401Proje\src\main.py", line 69, in analyze_data
    top_movies = data_loader.get_top_rated_movies(ratings_df, movies_df)
AttributeError: 'DataLoader' object has no attribute 'get_top_rated_movies'
2025-02-18 11:23:02,850 - INFO - Loading and preprocessing data...
2025-02-18 11:23:02,851 - INFO - Loading ratings data...
2025-02-18 11:23:28,770 - INFO - 
Validating ratings DataFrame:
2025-02-18 11:23:32,206 - INFO - Shape: 20000263 rows, 4 columns
2025-02-18 11:25:43,298 - INFO - Loading movies data...
2025-02-18 11:25:43,824 - INFO - 
Validating movies DataFrame:
2025-02-18 11:25:43,979 - INFO - Shape: 27278 rows, 3 columns
2025-02-18 11:25:46,670 - INFO - Loading tags data...
2025-02-18 11:25:47,480 - INFO - 
Validating tags DataFrame:
2025-02-18 11:25:47,741 - INFO - Shape: 465564 rows, 4 columns
2025-02-18 11:25:49,600 - WARNING - Column timestamp has 3 null values
2025-02-18 11:25:53,371 - INFO - Loading genome scores data...
2025-02-18 11:26:02,678 - INFO - 
Validating genome scores DataFrame:
2025-02-18 11:26:04,408 - INFO - Shape: 11709768 rows, 3 columns
2025-02-18 11:26:59,316 - INFO - Loading genome tags data...
2025-02-18 11:26:59,688 - INFO - 
Validating genome tags DataFrame:
2025-02-18 11:26:59,806 - INFO - Shape: 1128 rows, 2 columns
2025-02-18 11:27:01,273 - INFO - 
Dataset sizes:
2025-02-18 11:27:05,346 - INFO - Ratings: 20,000,263 rows
2025-02-18 11:27:05,559 - INFO - Movies: 27,278 rows
2025-02-18 11:27:05,794 - INFO - Tags: 465,564 rows
2025-02-18 11:27:07,554 - INFO - Genome Scores: 11,709,768 rows
2025-02-18 11:27:07,750 - INFO - Genome Tags: 1,128 rows
2025-02-18 11:27:07,751 - INFO - Analyzing data...
2025-02-18 11:27:07,850 - INFO - 
Movie rating statistics:
2025-02-18 11:27:20,791 - INFO - 
Top rated movies:
2025-02-18 11:27:34,500 - INFO - 
Genre statistics:
2025-02-18 11:27:47,865 - INFO - 
User rating statistics:
2025-02-18 11:28:00,284 - INFO - Training and evaluating model...
2025-02-18 11:28:00,285 - INFO - Starting model training with cross validation...
2025-02-18 11:28:00,346 - INFO - Training model...
2025-02-18 11:37:21,103 - ERROR - An error occurred: An error occurred while calling o1165.fit.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 502.0 failed 1 times, most recent failure: Lost task 6.0 in stage 502.0 (TID 1321) (kubernetes.docker.internal executor driver): java.io.FileNotFoundException: C:\Users\efeba\AppData\Local\Temp\blockmgr-2c839259-b8fd-4cc0-9808-3ef7b262a550\01\temp_shuffle_228499a4-f559-4b67-aa2d-c40cc8ac84ee (Sistem belirtilen yolu bulamýyor)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(Unknown Source)
	at java.io.FileOutputStream.<init>(Unknown Source)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:147)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:167)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:330)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1296)
	at org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:1090)
	at org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:737)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)
	at org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.FileNotFoundException: C:\Users\efeba\AppData\Local\Temp\blockmgr-2c839259-b8fd-4cc0-9808-3ef7b262a550\01\temp_shuffle_228499a4-f559-4b67-aa2d-c40cc8ac84ee (Sistem belirtilen yolu bulamýyor)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(Unknown Source)
	at java.io.FileOutputStream.<init>(Unknown Source)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:147)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:167)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:330)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	... 1 more
Traceback (most recent call last):
  File "C:\Users\efeba\BIL401Proje\src\main.py", line 129, in main
    best_model = train_and_evaluate_model(model_trainer, ratings_df)
  File "C:\Users\efeba\BIL401Proje\src\main.py", line 90, in train_and_evaluate_model
    best_model, predictions, rmse, mae = model_trainer.train_with_cross_validation(ratings_df)
  File "C:\Users\efeba\BIL401Proje\src\model_trainer.py", line 58, in train_with_cross_validation
    cv_model = cv.fit(training)
  File "C:\spark\python\pyspark\ml\base.py", line 205, in fit
    return self._fit(dataset)
  File "C:\spark\python\pyspark\ml\tuning.py", line 847, in _fit
    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):
  File "C:\Program Files\Python310\lib\multiprocessing\pool.py", line 873, in next
    raise value
  File "C:\Program Files\Python310\lib\multiprocessing\pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "C:\spark\python\pyspark\ml\tuning.py", line 847, in <lambda>
    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):
  File "C:\spark\python\pyspark\util.py", line 342, in wrapped
    return f(*args, **kwargs)
  File "C:\spark\python\pyspark\ml\tuning.py", line 113, in singleTask
    index, model = next(modelIter)
  File "C:\spark\python\pyspark\ml\base.py", line 98, in __next__
    return index, self.fitSingleModel(index)
  File "C:\spark\python\pyspark\ml\base.py", line 156, in fitSingleModel
    return estimator.fit(dataset, paramMaps[index])
  File "C:\spark\python\pyspark\ml\base.py", line 203, in fit
    return self.copy(params)._fit(dataset)
  File "C:\spark\python\pyspark\ml\wrapper.py", line 381, in _fit
    java_model = self._fit_java(dataset)
  File "C:\spark\python\pyspark\ml\wrapper.py", line 378, in _fit_java
    return self._java_obj.fit(dataset._jdf)
  File "C:\spark\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\spark\python\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
  File "C:\spark\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o1165.fit.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 502.0 failed 1 times, most recent failure: Lost task 6.0 in stage 502.0 (TID 1321) (kubernetes.docker.internal executor driver): java.io.FileNotFoundException: C:\Users\efeba\AppData\Local\Temp\blockmgr-2c839259-b8fd-4cc0-9808-3ef7b262a550\01\temp_shuffle_228499a4-f559-4b67-aa2d-c40cc8ac84ee (Sistem belirtilen yolu bulamýyor)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(Unknown Source)
	at java.io.FileOutputStream.<init>(Unknown Source)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:147)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:167)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:330)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1296)
	at org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:1090)
	at org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:737)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)
	at org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.FileNotFoundException: C:\Users\efeba\AppData\Local\Temp\blockmgr-2c839259-b8fd-4cc0-9808-3ef7b262a550\01\temp_shuffle_228499a4-f559-4b67-aa2d-c40cc8ac84ee (Sistem belirtilen yolu bulamýyor)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(Unknown Source)
	at java.io.FileOutputStream.<init>(Unknown Source)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:147)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:167)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:330)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	... 1 more

2025-02-18 11:42:41,400 - ERROR - KeyboardInterrupt while sending command.
Traceback (most recent call last):
  File "C:\Program Files\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] Varolan bir baðlantý uzaktaki bir ana bilgisayar tarafýndan zorla kapatýldý

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\spark\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\spark\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Program Files\Python310\lib\socket.py", line 705, in readinto
    return self._sock.recv_into(b)
KeyboardInterrupt
